{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Logistic regression with JAX GPU acceleration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0wriOaocASB",
        "outputId": "8f0130b7-7ecc-44a5-cf34-607bf47f8d43"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder_dir = '/content/drive/My Drive/Colab Notebooks/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9cEIiZVDPAv",
        "outputId": "1a98e08e-606d-4d88-b015-e3f2b90cbb81"
      },
      "source": [
        "from jax import jit,grad,vmap,device_put,random\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Setup datasets\n",
        "\n",
        "df_kminst_train = pd.read_csv(folder_dir+'k49_train.csv')\n",
        "df_kminst_test = pd.read_csv(folder_dir+'k49_test.csv')\n",
        "\n",
        "df_kminst_train = df_kminst_train.iloc[: , 1:]\n",
        "df_kminst_test = df_kminst_test.iloc[: , 1:]\n",
        "\n",
        "y_train = df_kminst_train['label'].to_numpy()\n",
        "y_test = df_kminst_test['label'].to_numpy()\n",
        "\n",
        "X_train = df_kminst_train.loc[:, df_kminst_train.columns != 'label'].to_numpy()\n",
        "X_test = df_kminst_test.loc[:, df_kminst_test.columns != 'label'].to_numpy()\n",
        "\n",
        "print('X_train:', X_train.shape,'Y_train:', y_train.shape,'X_test:', X_test.shape,'Y_test:', y_test.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (232365, 784) Y_train: (232365,) X_test: (38547, 784) Y_test: (38547,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLEc08pPd3-w"
      },
      "source": [
        "# Logistic regression with JAX class\n",
        "\n",
        "class JaxReg:\n",
        "    \"\"\"\n",
        "    Logistic regression classifier with GPU acceleration support through Google's JAX. The point of this class is fitting speed: I want this \n",
        "    to fit a model for very large datasets (k49 in particular) as quickly as possible!\n",
        "\n",
        "    - jit compilation utilized in sigma and loss methods (strongest in sigma due to matrix mult.). We need to 'partial' the \n",
        "      jit function because it is used within a class.\n",
        "\n",
        "    - jax.numpy (jnp) operations are JAX implementations of numpy functions. \n",
        "\n",
        "    - jax.grad used as the gradient function. Returns gradient with respect to first parameter.\n",
        "\n",
        "    - jax.vmap is used to 'vectorize' the jax.grad function. Used to compute gradient of batch elements at once, in parallel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=.001, num_epochs = 50, size_batch = 20):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.size_batch = size_batch     \n",
        "        \n",
        "    def fit(self, data, y):\n",
        "        self.K = max(y)+1 \n",
        "        ones = jnp.ones((data.shape[0],1))\n",
        "        X = jnp.concatenate((ones, data), axis = 1) \n",
        "        W = jnp.zeros((jnp.shape(X)[1], max(y)+1))\n",
        "\n",
        "        self.coeff = self.mb_gd(W, X, y)\n",
        "\n",
        "    # New mini-batch gradient descent function (because jitted functions require arrays which do not change shape)\n",
        "    def mb_gd(self, W, X, y):\n",
        "        num_epochs = self.num_epochs\n",
        "        size_batch = self.size_batch\n",
        "        eta = self.learning_rate\n",
        "        N = X.shape[0]\n",
        "      \n",
        "        # Define the gradient function using jit, vmap, and the jax's own gradient function, grad.\n",
        "        # vmap is especially useful for mini-batch GD since we compute all gradients of the batch at once, in parallel.\n",
        "        # Special paramaters in_axes,out_axes define the axis of the input paramters (W, X, y) and output (gradients of batches) \n",
        "        # upon which to vectorize. grads_b = loss_grad(W, X_batch, y_batch) has shape (batch_size, p+1, k) for p variables and k classes.\n",
        "         \n",
        "        loss_grad = jit(vmap(grad(self.loss), in_axes=(None, 0, 0), out_axes=0))\n",
        "        \n",
        "        for e in range(num_epochs):\n",
        "            shuffle_index = random.permutation(random.PRNGKey(e), N)\n",
        "            start_time = time.time()\n",
        "            for m in range(0, N, size_batch):\n",
        "                i = shuffle_index[m:m+size_batch]\n",
        "                \n",
        "                grads_b = loss_grad(W, X[i,:], y[i]) # 3D jax array of size (batch_size, p+1, k): gradients for each batch element\n",
        "\n",
        "                W -= eta * jnp.mean(grads_b, axis = 0) # Update W with average over each batch \n",
        "\n",
        "            epoch_time = time.time() - start_time # Epoch timer   \n",
        "            if e % 10 == 0:\n",
        "                print(\"Time to complete epoch\",e,\":\", epoch_time)\n",
        "        return W\n",
        "          \n",
        "    def predict(self, data):\n",
        "        ones = jnp.ones((data.shape[0],1)) \n",
        "        X = jnp.concatenate((ones, data), axis = 1)\n",
        "        W = self.coeff \n",
        "        y_pred = jnp.argmax(self.sigma(X,W), axis =1) \n",
        "        return y_pred\n",
        "    \n",
        "    def score(self, data, y_true):\n",
        "        ones = jnp.ones((data.shape[0],1))\n",
        "        X = jnp.concatenate((ones, data), axis = 1) \n",
        "        y_pred = self.predict(data) \n",
        "        acc = jnp.mean(y_pred == y_true) \n",
        "        return acc\n",
        "    \n",
        "    # jitting 'sigma' is the biggest speed-up compared to the original implementation \n",
        "    @partial(jit, static_argnums = 0)\n",
        "    def sigma(self, X, W): \n",
        "        if X.ndim == 1:\n",
        "            X = jnp.reshape(X, (-1, X.shape[0])) # jax.grad seems to necessitate a reshape: X -> (1,p+1)\n",
        "        s = jnp.exp(jnp.matmul(X, W))\n",
        "        total = jnp.sum(s, axis=1).reshape(-1,1)\n",
        "        return s/total\n",
        "\n",
        "    @partial(jit, static_argnums = 0)\n",
        "    def loss(self, W, X, y):\n",
        "        f_value = self.sigma(X, W)\n",
        "        loss_vector = jnp.zeros(X.shape[0])\n",
        "        for k in range(self.K):\n",
        "            loss_vector += jnp.log(f_value+1e-10)[:,k] * (y == k)\n",
        "        return -jnp.mean(loss_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGg8_IaqyRrl",
        "outputId": "c5ce58f8-2d31-42f5-f065-d48dfa0f7a0e"
      },
      "source": [
        "from jax.lib import xla_bridge\n",
        "\n",
        "# Find fitting times for JaxReg models using 20 epochs\n",
        "\n",
        "print(xla_bridge.get_backend().platform) # Confirm GPU in use\n",
        "\n",
        "# Commit data to device - note these are now JAX arrays. Type: jaxlib.xla_extension.DeviceArray\n",
        "X_train_dp = device_put(X_train)\n",
        "y_train_dp = device_put(y_train)\n",
        "\n",
        "batches = [2**i for i in range(7, 14)] # Batch sizes are powers of two\n",
        "times_jax = []\n",
        "\n",
        "for x in batches:\n",
        "    lg_sgd_jax = JaxReg(learning_rate=1e-6, num_epochs = 20, size_batch = x)\n",
        "    t0 = time.time()\n",
        "    lg_sgd_jax.fit(X_train_dp, y_train_dp)\n",
        "    t1 = time.time()\n",
        "    times_jax.append((t1-t0)/60)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpu\n",
            "Time to complete epoch 0 : 18.21995520591736\n",
            "Time to complete epoch 10 : 10.659940719604492\n",
            "Time to complete epoch 0 : 17.000915050506592\n",
            "Time to complete epoch 10 : 9.46127462387085\n",
            "Time to complete epoch 0 : 15.540739297866821\n",
            "Time to complete epoch 10 : 9.336594581604004\n",
            "Time to complete epoch 0 : 9.419742107391357\n",
            "Time to complete epoch 10 : 3.5257880687713623\n",
            "Time to complete epoch 0 : 9.296247720718384\n",
            "Time to complete epoch 10 : 3.497511386871338\n",
            "Time to complete epoch 0 : 9.085329532623291\n",
            "Time to complete epoch 10 : 3.462515354156494\n",
            "Time to complete epoch 0 : 10.040079832077026\n",
            "Time to complete epoch 10 : 3.3946666717529297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yeOZdk_Kpm0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O8YBOHLwfXt"
      },
      "source": [
        "# Logisitic regression classifier from Math 10 - unchanged\n",
        "\n",
        "class OldReg():\n",
        "    def __init__(self, learning_rate=.001, opt_method = 'SGD', num_epochs = 50, size_batch = 20):\n",
        "\n",
        "        # Initialize class attritubtes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.opt_method = opt_method\n",
        "        self.num_epochs = num_epochs\n",
        "        self.size_batch = size_batch     \n",
        "\n",
        "    def fit(self, data, y, n_iterations = 1000):\n",
        "        self.K = max(y)+1 # Number of classes = max val. + 1 (accounts for class label 0)\n",
        "        ones = np.ones((data.shape[0],1)) # Column of ones \n",
        "        X = np.concatenate((ones, data), axis = 1) # Augment data with ones array to account for intercept\n",
        "        eta = self.learning_rate\n",
        "        \n",
        "        W  = np.zeros((np.shape(X)[1],max(y)+1)) # Initialize W as empty (p+1) x k\n",
        "\n",
        "        # Gradient Descent \n",
        "        if self.opt_method == 'GD':\n",
        "            for k in range(n_iterations):\n",
        "                dW = self.loss_gradient(W,X,y) # Compute Gradient of loss function ( L(W;X,y) )\n",
        "                W = W - eta * dW # GD forumla: update W with gradient and learning rate\n",
        "        \n",
        "        # Stochastic Gradient Descent\n",
        "        if self.opt_method == 'SGD':\n",
        "            N = X.shape[0]\n",
        "            num_epochs = self.num_epochs\n",
        "            size_batch = self.size_batch\n",
        "\n",
        "            for e in range(num_epochs):\n",
        "                shuffle_index = np.random.permutation(N) # Reshuffle data every new epoch\n",
        "                start_time = time.time()\n",
        "                for m in range(0,N,size_batch): # m is the starting index of mini-batch\n",
        "                    i = shuffle_index[m:m+size_batch] # Index of samples in the mini-batch\n",
        "                    dW = self.loss_gradient(W,X[i,:],y[i]) # Data in mini-batch computes gradient\n",
        "                    W = W - eta * dW # GD formula\n",
        "                    \n",
        "                epoch_time = time.time() - start_time\n",
        "                print(\"Time to complete epoch\",e,\":\", epoch_time)\n",
        "            \n",
        "        self.coeff = W # Assign W once GD is complete\n",
        "\n",
        "    def predict(self, data):\n",
        "        ones = np.ones((data.shape[0],1)) \n",
        "        X = np.concatenate((ones, data), axis = 1) # Augment to account for intercept\n",
        "        W = self.coeff \n",
        "        y_pred = np.argmax(self.sigma(X,W), axis =1) # Predicted class is largest probability returned by softmax array\n",
        "        return y_pred\n",
        "    \n",
        "    def score(self, data, y_true):\n",
        "        ones = np.ones((data.shape[0],1))\n",
        "        X = np.concatenate((ones, data), axis = 1) \n",
        "        y_pred = self.predict(data) \n",
        "        acc = np.mean(y_pred == y_true) # Number of correct predictions/N\n",
        "        return acc\n",
        "    \n",
        "    def sigma(self,X,W): \n",
        "        s = np.exp(np.matmul(X,W)) # array e^zi\n",
        "        total = np.sum(s, axis=1).reshape(-1,1) # sum(e^z)\n",
        "        return s/total # Array of softmax probs\n",
        "    \n",
        "    def loss(self,W,X,y):\n",
        "        f_value = self.sigma(X,W) # Find soft-max prob. of X\n",
        "        K = self.K \n",
        "        loss_vector = np.zeros(X.shape[0])\n",
        "        for k in range(K):\n",
        "            loss_vector += np.log(f_value+1e-10)[:,k] * (y == k) # Compute loss. +1e-10 to  avoid nan issues\n",
        "        print(\"loss vec\", loss_vector.shape)\n",
        "        return -np.mean(loss_vector)               \n",
        "\n",
        "    def loss_gradient(self,W,X,y):\n",
        "        f_value = self.sigma(X,W)\n",
        "        K = self.K \n",
        "        dLdW = np.zeros((X.shape[1],K))\n",
        "        for k in range(K):\n",
        "            dLdWk =(f_value[:,k] - (y==k)).reshape(-1,1)*X # Numpy broadcasting\n",
        "            dLdW[:,k] = np.mean(dLdWk, axis=0)   # RHS is 1D Numpy array -- so you can safely put it in the k-th column of 2D array dLdW  \n",
        "        return dLdW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQkniqY9_x37",
        "outputId": "5dac6679-c05e-4914-91f5-25cbfffe0c29"
      },
      "source": [
        "import time\n",
        "from jax.lib import xla_bridge\n",
        "\n",
        "# Fit times for the old logisitic regression class using 20 epochs\n",
        "\n",
        "print(xla_bridge.get_backend().platform) # Also using GPU, but without acceleration\n",
        "\n",
        "times_old = []\n",
        "\n",
        "for x in batches:\n",
        "    lg_sgd_old = OldReg(learning_rate=1e-6, opt_method = 'SGD', num_epochs = 20, size_batch = x)\n",
        "    t0 = time.time()\n",
        "    lg_sgd_old.fit(X_train_dp, y_train_dp)\n",
        "    t1 = time.time()\n",
        "    times_old.append((t1-t0)/60)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpu\n",
            "Time to complete epoch 0 : 316.4438762664795\n",
            "Time to complete epoch 1 : 312.493798494339\n",
            "Time to complete epoch 2 : 311.62717604637146\n",
            "Time to complete epoch 3 : 313.05580711364746\n",
            "Time to complete epoch 4 : 312.63213181495667\n",
            "Time to complete epoch 5 : 312.3141930103302\n",
            "Time to complete epoch 6 : 311.9658670425415\n",
            "Time to complete epoch 7 : 311.8003704547882\n",
            "Time to complete epoch 8 : 312.7710795402527\n",
            "Time to complete epoch 9 : 313.3727157115936\n",
            "Time to complete epoch 10 : 312.7571358680725\n",
            "Time to complete epoch 11 : 312.08660197257996\n",
            "Time to complete epoch 12 : 311.5832562446594\n",
            "Time to complete epoch 13 : 312.4236171245575\n",
            "Time to complete epoch 14 : 314.6792256832123\n",
            "Time to complete epoch 15 : 311.4578011035919\n",
            "Time to complete epoch 16 : 311.6484405994415\n",
            "Time to complete epoch 17 : 313.5115809440613\n",
            "Time to complete epoch 18 : 312.38981556892395\n",
            "Time to complete epoch 19 : 314.74131178855896\n",
            "Time to complete epoch 0 : 166.48355674743652\n",
            "Time to complete epoch 1 : 165.63403224945068\n",
            "Time to complete epoch 2 : 165.0697853565216\n",
            "Time to complete epoch 3 : 165.1154887676239\n",
            "Time to complete epoch 4 : 165.15127062797546\n",
            "Time to complete epoch 5 : 164.71296620368958\n",
            "Time to complete epoch 6 : 163.19781637191772\n",
            "Time to complete epoch 7 : 164.40655779838562\n",
            "Time to complete epoch 8 : 163.9515779018402\n",
            "Time to complete epoch 9 : 165.08060598373413\n",
            "Time to complete epoch 10 : 163.765202999115\n",
            "Time to complete epoch 11 : 165.3684868812561\n",
            "Time to complete epoch 12 : 163.71972274780273\n",
            "Time to complete epoch 13 : 165.66123747825623\n",
            "Time to complete epoch 14 : 163.78877425193787\n",
            "Time to complete epoch 15 : 164.45327138900757\n",
            "Time to complete epoch 16 : 164.37683820724487\n",
            "Time to complete epoch 17 : 164.87250876426697\n",
            "Time to complete epoch 18 : 164.29962158203125\n",
            "Time to complete epoch 19 : 164.47406220436096\n",
            "Time to complete epoch 0 : 88.61623215675354\n",
            "Time to complete epoch 1 : 90.29638600349426\n",
            "Time to complete epoch 2 : 89.35876870155334\n",
            "Time to complete epoch 3 : 89.29397463798523\n",
            "Time to complete epoch 4 : 89.001131772995\n",
            "Time to complete epoch 5 : 89.80434155464172\n",
            "Time to complete epoch 6 : 89.73724794387817\n",
            "Time to complete epoch 7 : 90.28588914871216\n",
            "Time to complete epoch 8 : 88.70579385757446\n",
            "Time to complete epoch 9 : 89.48901128768921\n",
            "Time to complete epoch 10 : 89.9046835899353\n",
            "Time to complete epoch 11 : 90.04630589485168\n",
            "Time to complete epoch 12 : 88.76556658744812\n",
            "Time to complete epoch 13 : 89.5690553188324\n",
            "Time to complete epoch 14 : 89.70407247543335\n",
            "Time to complete epoch 15 : 90.03615522384644\n",
            "Time to complete epoch 16 : 89.51743268966675\n",
            "Time to complete epoch 17 : 91.38562846183777\n",
            "Time to complete epoch 18 : 89.13794159889221\n",
            "Time to complete epoch 19 : 89.60804486274719\n",
            "Time to complete epoch 0 : 50.331120014190674\n",
            "Time to complete epoch 1 : 50.27771878242493\n",
            "Time to complete epoch 2 : 51.65917634963989\n",
            "Time to complete epoch 3 : 50.78158473968506\n",
            "Time to complete epoch 4 : 50.88712215423584\n",
            "Time to complete epoch 5 : 50.65076804161072\n",
            "Time to complete epoch 6 : 50.0664484500885\n",
            "Time to complete epoch 7 : 50.03977870941162\n",
            "Time to complete epoch 8 : 51.61807870864868\n",
            "Time to complete epoch 9 : 50.93442392349243\n",
            "Time to complete epoch 10 : 52.81656789779663\n",
            "Time to complete epoch 11 : 51.33258366584778\n",
            "Time to complete epoch 12 : 51.07766127586365\n",
            "Time to complete epoch 13 : 50.311455726623535\n",
            "Time to complete epoch 14 : 52.24254298210144\n",
            "Time to complete epoch 15 : 50.91197419166565\n",
            "Time to complete epoch 16 : 50.909223318099976\n",
            "Time to complete epoch 17 : 52.38312840461731\n",
            "Time to complete epoch 18 : 52.68484330177307\n",
            "Time to complete epoch 19 : 52.17493772506714\n",
            "Time to complete epoch 0 : 32.051379919052124\n",
            "Time to complete epoch 1 : 32.55269956588745\n",
            "Time to complete epoch 2 : 33.002540826797485\n",
            "Time to complete epoch 3 : 32.275047063827515\n",
            "Time to complete epoch 4 : 33.85240340232849\n",
            "Time to complete epoch 5 : 32.75395655632019\n",
            "Time to complete epoch 6 : 33.38581824302673\n",
            "Time to complete epoch 7 : 33.0164053440094\n",
            "Time to complete epoch 8 : 33.29854464530945\n",
            "Time to complete epoch 9 : 32.364776611328125\n",
            "Time to complete epoch 10 : 33.38474369049072\n",
            "Time to complete epoch 11 : 34.193169832229614\n",
            "Time to complete epoch 12 : 32.521546840667725\n",
            "Time to complete epoch 13 : 33.24346876144409\n",
            "Time to complete epoch 14 : 32.510650634765625\n",
            "Time to complete epoch 15 : 33.304317474365234\n",
            "Time to complete epoch 16 : 33.290690183639526\n",
            "Time to complete epoch 17 : 33.291406869888306\n",
            "Time to complete epoch 18 : 32.308088541030884\n",
            "Time to complete epoch 19 : 32.430541038513184\n",
            "Time to complete epoch 0 : 27.84952163696289\n",
            "Time to complete epoch 1 : 28.05426263809204\n",
            "Time to complete epoch 2 : 28.791772842407227\n",
            "Time to complete epoch 3 : 28.199249267578125\n",
            "Time to complete epoch 4 : 27.554424285888672\n",
            "Time to complete epoch 5 : 28.306169748306274\n",
            "Time to complete epoch 6 : 27.4292631149292\n",
            "Time to complete epoch 7 : 27.84963893890381\n",
            "Time to complete epoch 8 : 27.92218041419983\n",
            "Time to complete epoch 9 : 27.898444890975952\n",
            "Time to complete epoch 10 : 28.73603892326355\n",
            "Time to complete epoch 11 : 28.410826444625854\n",
            "Time to complete epoch 12 : 29.62610650062561\n",
            "Time to complete epoch 13 : 29.599449396133423\n",
            "Time to complete epoch 14 : 29.161890983581543\n",
            "Time to complete epoch 15 : 29.62514853477478\n",
            "Time to complete epoch 16 : 29.26190733909607\n",
            "Time to complete epoch 17 : 29.556856870651245\n",
            "Time to complete epoch 18 : 29.44644570350647\n",
            "Time to complete epoch 19 : 29.115467071533203\n",
            "Time to complete epoch 0 : 26.88276743888855\n",
            "Time to complete epoch 1 : 26.27375602722168\n",
            "Time to complete epoch 2 : 26.665927171707153\n",
            "Time to complete epoch 3 : 26.353683710098267\n",
            "Time to complete epoch 4 : 25.780738592147827\n",
            "Time to complete epoch 5 : 26.106003046035767\n",
            "Time to complete epoch 6 : 25.990883827209473\n",
            "Time to complete epoch 7 : 25.73633599281311\n",
            "Time to complete epoch 8 : 25.459328174591064\n",
            "Time to complete epoch 9 : 26.048123359680176\n",
            "Time to complete epoch 10 : 25.34483790397644\n",
            "Time to complete epoch 11 : 25.68798327445984\n",
            "Time to complete epoch 12 : 25.725454807281494\n",
            "Time to complete epoch 13 : 25.419196605682373\n",
            "Time to complete epoch 14 : 26.245818853378296\n",
            "Time to complete epoch 15 : 25.4989492893219\n",
            "Time to complete epoch 16 : 25.690370321273804\n",
            "Time to complete epoch 17 : 25.87050771713257\n",
            "Time to complete epoch 18 : 26.044677734375\n",
            "Time to complete epoch 19 : 25.531667947769165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtcQH6XNqxcf",
        "outputId": "c7e57b0f-baf2-4e54-8c37-fb9f20795e29"
      },
      "source": [
        "# Compare speed ratios\n",
        "\n",
        "for i in range(len(times_jax)):\n",
        "    print(\"JAX regression\", round(times_old[i] / times_jax[i]), \"times as fast using batch size \", batches[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX regression 29 times as fast using batch size  128\n",
            "JAX regression 17 times as fast using batch size  256\n",
            "JAX regression 9 times as fast using batch size  512\n",
            "JAX regression 13 times as fast using batch size  1024\n",
            "JAX regression 9 times as fast using batch size  2048\n",
            "JAX regression 8 times as fast using batch size  4096\n",
            "JAX regression 7 times as fast using batch size  8192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "BHx7hL0a7M2R",
        "outputId": "faa0a4b5-64e6-437a-92c1-a78521e9936b"
      },
      "source": [
        "ind = np.arange(len(times_jax)) \n",
        "\n",
        "width = 0.35       \n",
        "plt.bar(ind, times_jax, width, label='JAX LR')\n",
        "plt.bar(ind + width, times_old, width,\n",
        "    label='Old LR')\n",
        "\n",
        "plt.ylabel('Time to fit 20 epochs (minutes)')\n",
        "plt.title('Logistic Regression with JAX GPU acceleration vs. standard')\n",
        "plt.xlabel('Batch size')\n",
        "plt.xticks(ind + width / 2, batches)\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd7wU5dn/8c9XAREFQSD+UMRjryFGSSwx9hijPlFTNDyxxxY1xpKoURM0MYktljRrjOVR7MaKJSoae1BRUIwVAQMWlACWCHL9/rjvg8u6u2f3nLPn7IHv+/Xa1+7U+9rZmbnmnpm9RxGBmZlZtRbr7ADMzKxrceIwM7OaOHGYmVlNnDjMzKwmThxmZlYTJw4zM6vJIpU4JJ0v6eetmG6IpNmSFq9HXI1K0ihJezdq+ZIulXRKR8a0KOvo5d3a7XVRI2mipG3bcX4habVK4zRs4mjvhQEQEQdHxK9qLTsiJkXE0hHxSS3lSdpH0ic56cyU9IyknVoTe2eIiG9ExGWNUH5elg+1ZX6lNoi8M5wraVBR/99Juruo3zmSbqsw/0GSLpL07/ybv5rnv1Ye3pRjmJ1fEyUdVzSsW4n4FvrkWOr3rXZ7bXTV7Ii7moZNHAuRRyNiaaAv8Gfgakl927uQRa021B4kLQV8G/gPsEfR4J8Dq0jaN4+7CbA3cHCZefUHHgF6AV8FegMbAA8AXysavW9eJ4YDv5C0fbt8oQZVnAytMbTpd4mIhnwBE4FtS/RfAjgH+Hd+nQMsUTD8GGBqHrY/EMBqedilwCn58wDgNmAG8C7wD1IivQKYB3wIzM7za8rz6ZanXRb4ay7jPeBvZb7DPsBDBd298ny+VPBdzgQmAW8C5wNL1vBdzgPuAN4HtgWWB24A3gZeAw4vmNeXgTHAzFzWWbl/T+D/gOl5WfwTWC4PGw3snz8vBpwIvA68BVwOLJOHNS+fvfN3eQc4ocwyWTmXs1juvgh4q2D4FcARheUDawMfAZ/k32RGwTL4E3A7MAt4HFi1wjo1f/nl7r2AycCPgfElxt8qL5eVgAnAQRXmfQrwTPP3KjPOAutR7vdP4CelhhWvsyXm92Xg0bw8pwJ/BHoUDF8XuIe0fr8JHJ/7Lw4cD7ySl9uTwIp52FoF0/wL2K1cLMBOwNhc/iPA0KLt91jgWeC/QDfguIIynwd2zeNW+n0LyzsAeDnHdguwfNFvezDwUo7nT4BKLLPlSdv2sgX9vkhaZ7sDq5GS/X9yv2uq3F+VnA54MMf2fv5uuwP9SPuet0n7j9uAwQXzGg38Cng4L6u7gQEFw/ckbYfTgRMo2FdWsU4EcGheTq/lfj/l0/3MfhRtJyW/b7U78o5+UT5x/BJ4DPgcMDCvsL/Kw7YHppE2mF6kHWK5xPFb0o66e359tXlFKy6bzyaO24Fr8grQHdiizHfYh5w4SBvrocDHwOdyv7NJG8CypCPUW4Hf1vBd/gN8hbRT70XaAfwC6AGsArwKfD2P/yiwZ/68NLBx/nxQLrdXjnFDoE/BCtycOPYjbbSr5OlvBK4oWj4XAUsCXyDtLNYus1wmARvmz//Kca5dMOyLJcqfvyyLdmTTSRtLN+BK4OoK61Rx4rgXOB1YDpjbHFPRNBeQdgT3U2JHVDDeY8BJLazT89cjQPm3+wDYhtYljg2BjfP8mkjJrTnp9ibtDI4mHRz0BjYq2FGMA9bMcXwB6A8sRUqk++Z5Nu9Q1ymx/XyRdACxUV5v9iZtN0sUbENjgRXJB0PAd0k77sVIO9D3gUEt/L7N5W2dY9mAdMD1B+DBot/2NlLNfghpp7x9meV2H3BAQfcZwPn580jSznixvNw2q3J/VXY6Prve9SfVdHvl3+U6Cg4+Sev9K8AapO1pNHBqHrYOKQFtnpfDWaR1tzlxlF0nCmK5h7TPWZK0n3kTWC///lcVx1vy+1azUDrjRfnE8QqwQ0H314GJ+fMl5B1v7l6N8onjl8DNpRZQcdksuMEPItVI+lXxHfbJP+oMYA7pSGe3PEykDWfVgvE34dOjgGq+y+UFwzcCJhWV/zPgr/nzg8DJFBy55P77UXS0WLQCN++47wUOKRi2Zv5OzStosOBR0xPA98oslyuAo4D/R0ocp5OOFotrI4Xl70PpHcvFBd07AC9U+D0Kl9+Q/Duun7vvAs4tMc0eeboDys03j/cycHBB9zfzd5kF3F20Hs0gHWlOINcKaUXiKBHDEcBN+fNw4Oky4/0L2LlE/92BfxT1uwAYUWL7OY98wFY03y0KtqH9Woh3bHMcFX7f5vL+ApxeMGzpvP41Ffy2hTvra4HjypS7P3BfwXY4Gdg8d18OXEjBulzlsi87HS3siIH1gfeKtrsTC7oPAe7Mn39BwcERaWf/MSX2lcXrREEsWxd0X0JOSrl7jZbijYgueY1jeVI1rdnruV/zsMkFwwo/FzuDtLHfnS9iHldl+SsC70bEe1WO/1hE9CXVTm4h1Wwg1ZZ6AU9KmiFpBnBn7g/VfZfCfisByzfPK8/veNLRNMAPSCvFC5L+WXCR/grSTvPqfFH3dEndS5RVarl3K5g/pBpSsw9IG3cpDwBbko6aHiRtKFvk1z8iYl6Z6UqptsxiewITImJs7r4S+N/C756vW5xJOh36yxauTU0nHVQAEBG35N/9SFINsNCAiOgXEWtHxO9zv7n5vXjZdyftID9D0hqSbpM0TdJM4DekU7CQ1tNXysRabthKwEZF69D3SQm+1LhHF427Ip9ui1C0zkraS9LYgvHXK4i3JQusfxExm7TMVygYp9p14QZgk3xDxOakA4h/5GHHkJLJE5Kek7RflfFVPZ2kXpIukPR6/t0eBPoWXacs910W2C9ExPuk5dA870rrRLPC36V4P/M6VeiKiePfpJW22ZDcD1LVfHDBsBXLzSQiZkXE0RGxCuno8ChJ2zQPrlD+ZGDZWi9w5xX9h8CekppPAXwIrBsRffNrmUgXTav9LoVxTibVVvoWvHpHxA65/JciYjjpFN9pwPWSloqIORFxckSsA2xKOm+9V4mySi33uaRqbq0eICXQLfPnh0inbbbI3aVU+k1aYy/Sxe9pkqaRqvwDSLWWZueQjvSOJG3cZ1aY373ALpJau01NJR9BF/VfmfIb83nAC8DqEdGHdKCgPGwy6bRiKZOBVcv0f6BoHVo6In5YZtxfF43bKyJGFowz/zeTtBLpVOZhQP+cVMcXxNvS77vA+pdvbOgPvNHCdJ+RD/ruJtWw/pd0BB952LSIOCAiliedxv1zNXdE1Tjd0aQa+0b5d9u8+WtVEf5UCvYFknqRlkOzSuvE/HDLzY+0Xbeo0RNHd0k9C17dSOcST5Q0UNIAUtXt//L41wL7Slo7L9Cy94BL2knSapJEulbwCenIA9LOsORGFxFTgVGkFaOfpO6SNi81bolp3wUuBn6Rj6ovAs6W9Lkc0wqSvl7rd8meAGZJOlbSkpIWl7SepC/lee8haWAud0aeZp6krSR9Ph/tzCTtvEod8Y8EjpS0sqSlSUcy10TE3BLjtrQcXiIlzT1IO6rmC/bfpnzieBMYLKn46L1m+Q6pVUnXRtbPr/VI53f3yuPsQLob6qg82Y9IiWGrMrM9i1SrvELSqkp653m3KNKt3jcAv5bUP69Xw0nntEeVmaw36TebrXTLb+EO/jZgkKQjJC0hqbekjfKwi4FfSVo9xzk0165uA9aQtGcuv7ukL0lau0TZFwEHS9ooz2MpSTvm71zKUqQd1tsASnerrVcwvKXfdyRpe1hf0hKk9e/xiJhYZvyWNP/W38mfyXF9V1LzAdt7OeYWa8AtTFe8P+lNWv9nSFoWGFFD3NcDO0naLC+rX7LgfrzSOlHKtcA+ktbJ+5mqYmn0xHEHaQE3v04i3b0yhnS3xjjgqdyPiBgF/J50IfNl0gVLSBdqi60O/J10oelR4M8RcX8e9ltScpoh6Sclpt2TtIN9gXSB8IgavtM5wA6ShpLuOnkZeCxXK/9OOhKp9bs073h2Iu2oXiPVaC4GlsmjbA88J2k2cC7p+sOHpNMQ15NWtgmkHfcVJYq4JPd/MM//I9LOtLUeAKZHxOSCbpF+z1LuA54Dpkl6pw3lBulC7s0RMS4fKU6LiGmk5bJTPjo+n3T94V2AiHiLdKR4oaQlPzPTiHdIFyU/ItWgZpHO4fem5Y232SGkO4aeJa1XhwE7RkS5Wt1PSEfMs0g78msK4plFSnz/Qzrt8RLpLjFISe5a0lH3TNL1gyXzNNsB3yMd4U8j1U6XKPF9x5DucvojaUf5Muk6RUkR8TzwO9K29ibwedJdQ80q/r4R8XfSwdMNpKPkVXOcrXULaR8wLSKeKej/JeDxvJ3cAvw4Il4FyKegvl9mfmWnI+23Lsv7k91I+4AlSdvoY6RT1FWJiOdIN9lcRVoO7wFTCkYpu06Umd+oHM99pN/wvmriaL6LaKGUj5TGk+70qPnIuJEsTN+lM0jqQ6pZ9ouIGS2Nb2blNXqNo2aSds1V836ko6Vbu+qOdmH6Lg1gd+AVJw2ztlvoEgfpwtRbpLtGPqH60wSNaGH6Lp1G0iOku5v27+xYzBYGC/WpKjMza38LY43DzMzqqEs3PjZgwIBoamrq7DDMzLqUJ5988p2IGNjymKV16cTR1NTEmDFjOjsMM7MuRVJV/xAvx6eqzMysJk4cZmZWEycOMzOrSZe+xmFmVmjOnDlMmTKFjz76qLNDaQg9e/Zk8ODBdO9eqsHr1nPiMLOFxpQpU+jduzdNTU2k9ksXXRHB9OnTmTJlCiuvvHK7ztunqsxsofHRRx/Rv3//RT5pAEiif//+dal9OXGY2ULFSeNT9VoWThxmZlYTX+Mws4VW03G3t+v8Jp66Y1XjLb300syePRuAI444guuuu47Jkyez2GLpWP2ss85i/PjxXHLJJQBceeWVXHXVVdx++4LxXnrppYwZM4Y//vGPC/Rvamqid+/eSKJfv35cfvnlrLTSSnQUJ47WOmmZlsepOP1/2icOM2tY8+bN46abbmLFFVfkgQceYKut0rO0Dj/8cIYNG8bDDz/Muuuuy4knnsi9995b07zvv/9+BgwYwIgRIzjllFO46KKL6vEVSvKpKjOzOhk9ejTrrrsuP/zhDxk58tPHsXfr1o0///nPHHrooRxzzDHst99+rLJKuUfEV7bJJpvwxhs1P3q9TVzjMDOrk5EjRzJ8+HB23nlnjj/+eObMmTP/PxWbbropa6+9Nn//+9+ZMGFCq8u488472WWXXdor5Kq4xmFmVgcff/wxd9xxB7vssgt9+vRho4024q677po/fPbs2YwZM4Y5c+bw9ttv1zz/rbbaihVWWIFRo0YxfPjw9gy9RU4cZmZ1cNdddzFjxgw+//nP09TUxEMPPbTA6aoRI0awxx57cMIJJ3DkkUfWPP/777+f119/nfXXX58RI0a0Z+gtqlvikHSJpLckjS/ot6ykeyS9lN/75f6S9HtJL0t6VtIG9YrLzKwjjBw5kosvvpiJEycyceJEXnvtNe655x4++OADxo0bx+23386xxx7LgQceyMSJE7nnnntqLqNbt26cc845XH755bz77rt1+BZlyq3jvC8F/ghcXtDvOODeiDhV0nG5+1jgG8Dq+bURcF5+NzNrtWpvn21Pc+fOpXv37tx5552cf/758/svtdRSbLbZZtx666384Q9/4Oyzz6Znz54AnHfeeey1116MHTuWHj16LDC/Sy+9lL/97W/zux977LEFhg8aNIjhw4fzpz/9iZ///Od1/GafquszxyU1AbdFxHq5+1/AlhExVdIgYHRErCnpgvx5ZPF4leY/bNiw6LQHOfl2XLOGM2HCBNZee+1OjeGZZ57hgAMO4IknnujUOJqVWiaSnoyIYa2dZ0df41iuIBlMA5bLn1cAJheMNyX3+wxJB0oaI2lMay4omZnVy/nnn8/w4cM55ZRTOjuUuuq0i+ORqjo1V3ci4sKIGBYRwwYObPUjc83M2t3BBx/M888/z3bbbdfZodRVRyeON/MpKvL7W7n/G8CKBeMNzv3MzKzBdHTiuAXYO3/eG7i5oP9e+e6qjYH/tHR9w8zMOkfd7qqSNBLYEhggaQowAjgVuFbSD4DXgd3y6HcAOwAvAx8A+9YrLjMza5u6JY6IKPdXxm1KjBvAofWKxczM2o/bqjKzhVdbb5v/zPxavo1+ypQpHHrooTz//PPMmzePnXbaiTPOOIMePXowevRozjzzTG677bbPTNfU1MSYMWMYMGBAi/0vvfRSfvrTn7LCCivw0UcfcdBBB7Xq3+et5SZHzMzaSUTwrW99i1122YWXXnqJF198kdmzZ3PCCSe0e1m77747Y8eO5eGHH+bXv/41kydPbnmiduLEYWbWTu677z569uzJvvumy7SLL744Z599NpdccgkffPDBAuNOnz6d7bbbjnXXXZf999+f1v4Zu3///qy22mpMndpx9xM5cZiZtZPnnnuODTfccIF+ffr0YciQIbz88ssL9D/55JPZbLPNeO6559h1112ZNGlSq8qcNGkSH330EUOHDm113LVy4jAz6wQPPvgge+yxBwA77rgj/fr1q2n6a665hqFDh7LaaqtxyCGHzG/3qiM4cZiZtZN11lmHJ598coF+M2fOZNKkSay22mrtWtbuu+/Os88+yyOPPMJxxx3HtGnT2nX+lThxmJm1k2222YYPPviAyy9PjYJ/8sknHH300eyzzz706tVrgXE333xzrrrqKgBGjRrFe++916oyhw0bxp577sm5557btuBr4NtxzWzh1cGtUEvipptu4pBDDuFXv/oV8+bNY4cdduA3v/nNZ8YdMWIEw4cPZ91112XTTTdlyJAhZec7dOhQFlssHefvtttun7meceyxx7LBBhtw/PHH07t37/b9UiVUbFZd0mDge8BXgeWBD4HxwO3AqIiYV/cIK3Cz6mZWqBGaVW809WhWvWyNQ9JfSU2b3wacRmqQsCewBrA9cIKk4yLiwdYWbmZmXU+lU1W/i4jxJfqPB26U1AMoX7cyM7OFUtmL46WShqR+kobm4R9HxMufndLMrPPU86mmXU29lkWLd1VJGi2pj6RlgaeAiySdXZdozMzaoGfPnkyfPt3Jg5Q0pk+fXpf/d1RzV9UyETFT0v7A5RExQtKz7R6JmVkbDR48mClTpuDHSic9e/Zk8ODB7T7fahJHt/y0vt2A9m+py8ysnXTv3p2VV165s8NY6FXzB8BfAncBr0TEPyWtArxU37DMzKxRtVjjiIjrgOsKul8Fvl3PoMzMrHFVc3F8DUn3Shqfu4dKOrH+oZmZWSOq5lTVRcDPgDkAEfEs6d/kZma2CKomcfSKiCeK+s2tRzBmZtb4qkkc70haFQgASd8BOu5RU2Zm1lCquR33UOBCYC1JbwCvAd+va1RmZtawqkkcERHbSloKWCwiZknyjdJmZouoak5V3QAQEe9HxKzc7/r6hWRmZo2sUrPqawHrAstI+lbBoD6k5tXNzGwRVOlU1ZrATkBf4H8K+s8CDqhnUGZm1rjKJo6IuBm4WdImEfFoB8ZkZmYNrJqL4wdK+kwNIyL2q0M8ZmbW4KpJHLcVfO4J7Ar8uz7hmJlZo6umkcMbCrsljQQeqltEZmbW0Kq5HbfY6sDn2jsQMzPrGqppHXeWpJnN78CtwLFtKVTSkZKekzRe0khJPSWtLOlxSS9LukZSj7aUYWZm9dFi4oiI3hHRp+B9jeLTV7WQtAJwODAsItYDFie1tnsacHZErAa8B/ygtWWYmVn9VHWqStIKkjaVtHnzq43ldgOWlNQN6EVqNHFrPv1H+mXALm0sw8zM6qDFi+OSTgN2B54HPsm9A3iwNQVGxBuSzgQmAR8CdwNPAjMiorm59inACmXiORA4EGDIkCGtCcHMzNqgmttxdwHWjIj/tkeBkvoBOwMrAzNIj6XdvtrpI+JCUmu9DBs2LNojJjMzq141p6peBbq3Y5nbAq9FxNsRMQe4EfgK0DefugIYDLzRjmWamVk7qabG8QEwVtK9wPxaR0Qc3soyJwEbS+pFOlW1DTAGuB/4DnA1sDdwcyvnb2ZmdVRN4rglv9pFRDwu6XrgKdIjaJ8mnXq6Hbha0im531/aq0wzM2s/1fxz/LL2LjQiRgAjinq/Cny5vcsyM7P2Vel5HNdGxG6SxpGfN14oIobWNTIzM2tIlWocP87vO3VEIGZm1jVUeh7H1Pz+OoCkPpXGNzOzRUM1fwA8CDgZ+IhPT1kFsEod4zIzswZVTQ3iJ8B6EfFOvYMxM7PGV80fAF8h/ZfDzMysqhrHz4BHJD1O+/wB0MzMurBqEscFwH3AOGBefcMxM7NGV03i6B4RR9U9EjMz6xKqucYxStKBkgZJWrb5VffIzMysIVVT4xie339W0M+345qZLaKqaatq5Y4IxMzMuoayp6okbVZpQkl9JK3X/iGZmVkjq1Tj+Lak04E7SY92fRvoCawGbAWsBBxd9wjNzKyhVGqr6sh8EfzbwHeBQaQHL00ALoiIhzomRDMzayQVr3FExLvARfllZmZW1e24ZmZm8zlxmJlZTZw4zMysJi0mDknfldQ7fz5R0o2SNqh/aGZm1oiqqXH8PCJm5f91bAv8BTivvmGZmVmjqiZxfJLfdwQujIjbgR71C8nMzBpZNYnjDUkXALsDd0haosrpzMxsIVRNAtgNuAv4ekTMAJYFflrXqMzMrGG1mDgi4gPgZuB9SUOA7sAL9Q7MzMwaU4ut40r6ETACeJNPnwAYwNA6xmVmZg2qmudx/BhYMyKm1zsYMzNrfNVc45gM/KfegZiZWddQtsYhqfk5468CoyXdDvy3eXhEnFXn2MzMrAFVOlXVO79Pyq8e+P8bZmaLvErP4zi5XoVK6gtcDKxHutC+H/Av4BqgCZgI7BYR79UrBjMza51q2qq6J+/om7v7SbqrjeWeC9wZEWsBXyA9HOo44N6IWB24N3ebmVmDqebi+MD8xz8Aci3gc60tUNIywOakNq+IiI/z/HcGLsujXQbs0toyzMysfqpqqyr/8Q8ASSuRTi+11sqk55f/VdLTki6WtBSwXERMzeNMA5ZrQxlmZlYn1fyP4wTgIUkPAAK+ChzYxjI3AH4UEY9LOpei01IREZJKJidJBzaXP2TIkFKjmJlZHVXT5MidpB39NcDVwIYR0ZZrHFOAKRHxeO6+Ps//TUmDAPL7W2XiuTAihkXEsIEDB7YhDDMza41qW7ndFNgyvzZuS4ERMQ2YLGnN3Gsb4HngFmDv3G9vUvtYZmbWYKppq+pU4EvAlbnXjyVtGhHHt6HcHwFXSupB+oPhvqQkdq2kHwCvk1rlNTOzBlPNNY4dgPUjYh6ApMuAp4FWJ46IGAsMKzFom9bO08zMOka1p6r6Fnxeph6BmJlZ11BNjeO3wNOS7ifdVbU5/nOemdkiq8XEEREjJY0mXecI4Nh8gdvMzBZB1dQ4ADYBNiMljm7ATXWLyMzMGlo1bVX9GTgYGAeMBw6S9Kd6B2ZmZo2pmhrH1sDaEREw/66q5+oaldXPSW28t+EkP9PLbFFXzV1VLwOFbXusmPuZmdkiqJoaR29ggqQnSNc4vgyMkXQLQER8s47xmZlZg6kmcfyi7lGYmVmXUc3tuA/kptRXj4i/S1oS6BYRs+ofnpmZNZpq7qo6gNSC7QW512Dgb/UMyszMGlc1F8cPBb4CzASIiJdowxMAzcysa6smcfw3Ij5u7pDUjbY9AdDMzLqwahLHA5KOB5aU9DXgOuDW+oZlZmaNqprEcRzpGeHjgIOAO4AT6xmUmZk1rmruqpoHXJRfZma2iKv2eRxmZmaAE4eZmdXIicPMzGpSNnFIWkbSqZJekPSupOmSJuR+fctNZ2ZmC7dKNY5rgfeALSNi2YjoD2yV+13bEcGZmVnjqZQ4miLitMLHxEbEtIg4DVip/qGZmVkjqpQ4Xpd0jKTlmntIWk7SscDk+odmZmaNqFLi2B3oT/rn+LuS3gVGA8sCu3VAbGZm1oDK/gEwIt4Djs0vMzMzoIXbcSWtJWkbSUsV9d++vmGZmVmjqnQ77uHAzcCPgOck7Vww+Df1DszMzBpTpbaqDgA2jIjZkpqA6yU1RcS5gDoiODMzazyVEsdiETEbICImStqSlDxWwonDzGyRVekax5uS1m/uyElkJ2AA8Pl6B2ZmZo2pUuLYC5hW2CMi5kbEXsDmdY3KzMwaVtnEERFTCv81XjTs4bYWLGlxSU9Lui13ryzpcUkvS7pGUo+2lmFmZu2vM1vH/TEwoaD7NODsiFiN1B7WDzolKjMzq6hTEoekwcCOwMW5W8DWwPV5lMuAXTojNjMzq6zFxCHptGr61egc4BhgXu7uD8yIiLm5ewqwQpl4DpQ0RtKYt99+u41hmJlZraqpcXytRL9vtLZASTsBb0XEk62ZPiIujIhhETFs4MCBrQ3DzMxaqez/OCT9EDgEWEXSswWDegNtuTj+FeCbknYAegJ9gHOBvpK65VrHYOCNNpRhZmZ1UqnGcRXwP8At+b35tWFE7NHaAiPiZxExOCKagO8B90XE94H7ge/k0fYmNXdiZmYNplLiiIiYCBwKzCp4IWnZOsRyLHCUpJdJ1zz+UocyzMysjSo1OXIV6Z/iTwLBgs2MBLBKWwuPiNGkZ3wQEa8CX27rPM3MrL4qJY5T8/vaEfFRRwRjZmaNr9KpqnPz+yMdEYiZmXUNlWoccyRdCAyW9PvigRFxeP3CMjOzRlUpcewEbAt8nXSdw8zMrOIzx98BrpY0ISKe6cCYzMysgbX4z3EnDTMzK9SZreOamVkX5MRhZmY1qaZ13GUknd3cIq2k30lapiOCMzOzxlNNjeMSYCawW37NBP5az6DMzKxxVbodt9mqEfHtgu6TJY2tV0BmZtbYqqlxfChps+YOSV8BPqxfSGZm1siqqXEcDFxecF3jPVKz52ZmtgiqJnHMjIgvSOoDEBEzJa1c57jMzKxBVZM4bgA2iIiZBf2uBzasT0hmRU5q4018J/2nfeIwM6Dyo2PXAtYFlpH0rYJBfUiPfDUzs0VQpRrHmqSGDvuSHhnbbBZwQD2DMjOzxlWpkcObgZslbRIRjwCi9FYAAA38SURBVHZgTGZm1sCqaeTQScPMzOZzW1VmZlYTJw4zM6tJNY0cLifpL5JG5e51JP2g/qGZmVkjqqbGcSlwF7B87n4ROKJeAZmZWWOrJnEMiIhrgXkAETEX+KSuUZmZWcOqJnG8L6k/EACSNgb8V1wzs0VUNU2OHAXcAqwq6WFgIPCdukZlZmYNq8XEERFPSdqC9E9yAf+KiDl1j8zMzBpSi4lD0uLADkBTHn87SUTEWXWOzczMGlA1p6puBT4CxpEvkJuZ2aKrmsQxOCKG1j0SMzPrEqq5q2qUpO3qHomZmXUJ1SSOx4CbJH0oaaakWZJmtjhVGZJWlHS/pOclPSfpx7n/spLukfRSfu/X2jLMzKx+qkkcZwGbAL0iok9E9I6IPm0ocy5wdESsA2wMHCppHeA44N6IWB24N3ebmVmDqSZxTAbGR0S0R4ERMTUinsqfZwETgBWAnYHL8miXAbu0R3lmZta+qrk4/iowOjdy+N/mnu1xO66kJuCLwOPAchExNQ+aBixXZpoDgQMBhgwZ0tYQzMysRtXUOF4jnTrqAfQueLWJpKWBG4AjImKBaya5dlOyhhMRF0bEsIgYNnDgwLaGYWZmNarmn+Mnt3ehkrqTksaVEXFj7v2mpEERMVXSIOCt9i7XzMzarmzikPTHiDhM0q2UOPqPiG+2pkBJAv4CTCg63XULsDdwan6/uTXzNzOz+qpU49gLOAw4s53L/AqwJzBO0tjc73hSwrg2PyTqdWC3di7XzMzaQaXE8QpARDzQngVGxEOkxhJL2aY9yzIzs/ZXKXEMlHRUuYFu5NCsgpOWaeP0fuSNNa5KiWNxYGnK1w7MzGwRVClxTI2IX3ZYJGZm1iVU+h+HaxpmZvYZlRKHL1SbmdlnlE0cEfFuRwZiZmZdQzVNjpiZmc3nxGFmZjVx4jAzs5o4cZiZWU2cOMzMrCZOHGZmVpNqngBoZosKt7FlVXCNw8zMauLEYWZmNfGpKjPr2nx6rcO5xmFmZjVxjcPMrDN04ZqSaxxmZlYTJw4zM6uJE4eZmdXEicPMzGrixGFmZjVx4jAzs5o4cZiZWU2cOMzMrCZOHGZmVpNF9p/jTcfd3qbpJ/Zsp0Bq1BXj7ooxm1l5rnGYmVlNnDjMzKwmThxmZlaThrrGIWl74FxgceDiiDi1k0OyRVhbrs34Glj1umLM0HXjbg8NU+OQtDjwJ+AbwDrAcEnrdG5UZmZWrGESB/Bl4OWIeDUiPgauBnbu5JjMzKyIIqKzYwBA0neA7SNi/9y9J7BRRBxWNN6BwIG5c03gXx0a6KcGAO90Utlt0RXjdswdpyvG3RVjhs6Ne6WIGNjaiRvqGkc1IuJC4MLOjkPSmIgY1tlx1Korxu2YO05XjLsrxgxdN25orFNVbwArFnQPzv3MzKyBNFLi+CewuqSVJfUAvgfc0skxmZlZkYY5VRURcyUdBtxFuh33koh4rpPDqqTTT5e1UleM2zF3nK4Yd1eMGbpu3I1zcdzMzLqGRjpVZWZmXYATh5mZ1cSJowxJl0h6S9L4gn5nSHpB0rOSbpLUN/fvLukySeMkTZD0s06KeUVJ90t6XtJzkn6c+58k6Q1JY/Nrh4Jphkp6NI8/TlKHN4QgaWIue6ykMbnfd3NM8yQNKxj3a5KezOM/KWnrDoyz1DqxrKR7JL2U3/vl/t/P68k4SY9I+kLRvBaX9LSk2+occ7l1omTcBdN9SdLc/P+q5n6n53lMkPR7Sapz7Asso3zjzOOSXpZ0Tb6JBkkrSbo3L+/RkgYXzGOIpLtzzM9LaqpzzEfmZTRe0khJPSUdlmMOSQMKxu2X9yPPSnpC0nq5f8nfrKFEhF8lXsDmwAbA+IJ+2wHd8ufTgNPy5/8Frs6fewETgaZOiHkQsEH+3Bt4kdR8y0nAT0qM3w14FvhC7u4PLN4JcU8EBhT1W5v0B8/RwLCC/l8Els+f1wPe6OR14nTguPz5uIJ1YlOgX/78DeDxonkdBVwF3NZJ60TJuHP34sB9wB3Adwq+z8N52OLAo8CWdY59gWUEXAt8L38+H/hh/nwdsHf+vDVwRcE8RgNfy5+XBnrVMd4VgNeAJQvi3Sevs03F6zlwBjAif14LuLfSb9ZR63k1L9c4yoiIB4F3i/rdHRFzc+djpP+aAASwlKRuwJLAx8DMjoq1IL6pEfFU/jwLmEBamcvZDng2Ip7J00yPiE/qH2nLImJCRHymVYCIeDoi/p07nwOWlLREB8X0mXWC1CzOZfnzZcAuedxHIuK93L9wXSEfEe8IXFzXgKm4TpSMO/sRcAPwVuGsgJ5AD2AJoDvwZr3iLl5GuXazNXB9iZjXISU6gPvJTRUptXXXLSLuAYiI2RHxQb1izrqR1slupIPIf+d1dmKJcefHHREvAE2SlmvFdtzhnDhabz9gVP58PfA+MBWYBJwZEcU7mA6Vq+RfBB7PvQ7LVeJLCk5LrAGEpLskPSXpmE4IFdJO6e586unAFsf+1LeBpyLiv3WKqxrLRcTU/HkasFyJcX7Ap+sKwDnAMcC8Ose2gKJ1omTcklYAdgXOK5w2Ih4l7ZSn5tddETGhjuEWL6P+wIyCA7cpfLozfQb4Vv68K9BbUn/S+j1D0o35lNcZSo2p1kVEvAGcSdoHTAX+ExF3V5hkftySvgysRMEBRu7fxILbcUNw4mgFSScAc4Erc68vA58AywMrA0dLWqWTwkPS0qQjxiMiYiZpJ7AqsD5phf5dHrUbsBnw/fy+q6RtOj5iNouIDUindA6VtHlLE0hal3S68KB6B1etSOcWFri/XdJWpMRxbO7eCXgrIp7syNhKrBPzFcV9DnBsRMwrmn410unDwaQd9taSvlqnWGtdRj8BtpD0NLAFqcWJT0jr91fz8C8Bq5BOHdVFPiDbmbQPWJ50FmKPCpOcCvSVNJZUy3s6x908v7K/WWdrmD8AdhWS9gF2ArbJGxykaxx3RsQc4C1JDwPDgFc7Ib7upJXtyoi4ESAi3iwYfhHQfEF2CvBgRLyTh91BOod/b0fGnI/UiIi3JN1ESsQPlhs/n8a4CdgrIl7pmCjLelPSoIiYKmkQBad3JA0lnWr5RkRMz72/AnxT6QaFnkAfSf8XEZV2MG1Sap2oEPcw4Op83XsAsIOkucDqwGMRMTvPcxSwCfCPOoT8mWVEek5PX0ndcq1jfpNE+dRl85H70sC3I2KGpCnA2Ih4NQ/7G7Ax8Jc6xAywLfBaRLydy7uRdG3o/0qNnJPBvnlcka6PNMda6jdrGK5x1EDpQVPHAN8sOlc6iXT+FUlLkVbOFzohPpE2igkRcVZB/0EFo+0KNN8VdBfweUm98jnZLYDnOyreHNtSkno3fyZddxlfYfy+wO2kC7sPd0yUFd0C7J0/7w3cDOluHuBGYM+IeLF55Ij4WUQMjogmUrM699U5aZRcJ8rFHRErR0RTju964JCI+BtpHd9CUre8U9uCdO693ZVZRt8nnSprvsurcFkPkNS8L/sZcEn+/E9SsmluBXZr6rt+TwI2ztuTgG2osIwk9W2+MwzYn3QQN7PCb9Y4OvvqfKO+gJGk0zpzSEfmPwBeBiYDY/Pr/Dzu0qQ7O54jrZg/7aSYNyOdcni2IMYdgCuAcbn/LcCggmn2yHGPB07vhJhXIZ3rfSbHcULuv2te7v8lXYS9K/c/kXQ9aWzB63OduE70J9XQXgL+Diybx70YeK8gxjEl5rcl9b+rqtw6UTLuomkv5dO7qhYHLiDtCJ8HzuqgZT5/GeV15Ym8HV4HLJH7fyd/jxfzcl+iYPqv5e8+Ln+fHnWO92TSQeP4vN0tARye15e5wL9JTzeFVGN7kfRoiBv59C68kr9ZRyzval9ucsTMzGriU1VmZlYTJw4zM6uJE4eZmdXEicPMzGrixGFmZjVx4rBFkqRPlFrjfSY3t7JpC+P3lXRIFfMdrYLWfGuM6Y78PxWzhubEYYuqDyNi/Yj4AulPY79tYfy+QIuJoy0iYoeImFHPMszagxOHWWrS4j1ITVYoPdvhKaVnaeycxzkVWDXXUs7I4x6bx3lG0qkF8/tufr7Ci6Xac5I0SNKDeV7jm8dRei7JAEkH69Nnp7wm6f48fDulZ6c8Jem63LyGWYfzHwBtkSTpE9K/iXuSnn+wdUQ8mZte6RWp6YcBpCbRVye1XHpbRDQ/bOcbwM+BbSPiA0nLRsS7kkYDT0bE0bmtpaMiYtuiso8GekbEr3Nrrb0iYpakiaRnjzS3Hdad1Oz26aTnX9xIavfqfUnHkv4h/ct6LiezUtzIoS2qPoyI9QEkbQJcrvQENgG/yS30ziO1BFuqqfRtgb9GbrMsFmxGv7lRuidJD/Ap9k/gkpwY/hYRY8vEeC6pnaZbc4ux6wAP5wYIe5CSiVmHc+KwRV5EPJprFwNJ7TgNBDaMiDm5FlDr43Sbnw/S3LR3cXkP5sS0I3CppLMi4vLCcXIrzCsBhzX3Au6JiOE1xmLW7nyNwxZ5ktYiNeI3HViG9CyIOUrP0lgpjzaL9BjPZvcA+0rqleexbA3lrQS8GREXkRrl26Bo+IakZ0jsEZ8+F+Mx4CtKz8VoblV4jdq+qVn7cI3DFlVLKj1AB9LR/N4R8YmkK4FbJY0DxpCbx4+I6ZIeljQeGBURP5W0PjBG0sek53MfX2XZWwI/lTQHmA3sVTT8MGBZ4P58WmpMROyfayEj9emjck8kta5q1qF8cdzMzGriU1VmZlYTJw4zM6uJE4eZmdXEicPMzGrixGFmZjVx4jAzs5o4cZiZWU3+P6v7h5OsYLoBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcL-Z_rWn1Pc",
        "outputId": "462310cc-86a8-46e1-9f2b-99171095f6dd"
      },
      "source": [
        "# They are almost equally accurate\n",
        "print(lg_sgd_old.score(X_test,y_test))\n",
        "print(lg_sgd_jax.score(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.49298259267906713\n",
            "0.49267128\n"
          ]
        }
      ]
    }
  ]
}